# ECG分类模型说明文档

## 概述

本项目实现了两种用于ECG（心电图）分类的深度学习模型，通过`Model`类统一封装，支持五种心律类型的分类：
- N: Normal（正常）
- A: Atrial Premature（房性早搏）
- V: Ventricular Premature（室性早搏）
- L: Left Bundle Branch Block（左束支传导阻滞）
- R: Right Bundle Branch Block（右束支传导阻滞）

## 模型架构对比

| 特征 | Model_1 | Model_2 |
|------|---------|---------|
| 架构类型 | 传统CNN | 深度残差网络（ResNet-like） |
| 卷积层数 | 4层 | 1层初始 + 1层残差 + 15×2层残差块 |
| 激活函数 | Tanh + ReLU | ReLU（全部） |
| 归一化 | 无 | BatchNorm |
| 残差连接 | 无 | 有 |
| 参数量 | 较少 | 较多 |
| 深度 | 中等 | 深 |

## Model_1 详细说明

### 网络结构

```
输入 (1×300)
│
├─ Conv1d(1→4, k=21, p=10)
│   └─ Tanh()
│   └─ MaxPool1d(k=3, s=2, p=1)  → (4×150)
│
├─ Conv1d(4→16, k=23, p=11)
│   └─ ReLU()
│   └─ MaxPool1d(k=3, s=2, p=1)  → (16×75)
│
├─ Conv1d(16→32, k=25, p=12)
│   └─ Tanh()
│   └─ AvgPool1d(k=3, s=2, p=1)  → (32×38)
│
├─ Conv1d(32→64, k=27, p=13)     → (64×19)
│   └─ ReLU()
│
├─ Flatten()                     → 1216
│
├─ Linear(1216→128)
│   └─ Dropout(0.2)
│
└─ Linear(128→5)                 → 5类输出
```

### 特点

1. **渐进式特征提取**
   - 逐渐增加卷积核大小（21→23→25→27）
   - 逐渐增加通道数（1→4→16→32→64）
   - 通过池化逐步降低时间维度

2. **混合激活函数**
   - 第一、三层使用Tanh
   - 第二、四层使用ReLU
   - 不同激活函数可能带来特征多样性

3. **多样化池化策略**
   - 第一、二层使用最大池化（保留显著特征）
   - 第三层使用平均池化（保留整体信息）

4. **轻量级设计**
   - 参数相对较少
   - 训练速度快
   - 适合资源受限的环境

### 优点

- **计算效率高**：网络结构简单，前向传播速度快
- **特征提取有效**：通过不同大小的卷积核捕获多尺度特征
- **易于训练**：没有深度网络的梯度消失问题
- **内存占用小**：适合部署在嵌入式设备

### 缺点

- **表达能力有限**：相比深度网络，可能难以学习非常复杂的模式
- **缺乏残差连接**：深层特征传递可能受限
- **无归一化层**：训练稳定性可能不如带BatchNorm的网络

## Model_2 详细说明

### 网络结构

```
输入 (1×300)
│
├─ 初始卷积块
│   ├─ Conv1d(1→64, k=7, s=2, p=3)     → (64×150)
│   ├─ BatchNorm1d(64)
│   └─ ReLU()
│
├─ 第一个残差块
│   ├─ Conv1d(64→64, k=3, p=1)
│   ├─ BatchNorm1d(64)
│   ├─ ReLU()
│   ├─ Dropout(0.2)
│   ├─ Conv1d(64→64, k=3, p=1)
│   ├─ 残差连接: input + output
│   └─ MaxPool1d(k=2, s=2, p=1)        → (64×76)
│
├─ 15个标准残差块（循环执行）
│   └─ 每个块包含:
│       ├─ BatchNorm1d(64)
│       ├─ ReLU()
│       ├─ Dropout(0.2)
│       ├─ Conv1d(64→64, k=3, p=1)
│       ├─ BatchNorm1d(64)
│       ├─ ReLU()
│       ├─ Dropout(0.2)
│       ├─ Conv1d(64→64, k=3, p=1)
│       ├─ 残差连接: input + output
│       └─ MaxPool1d(k=2, s=2, p=1)（如果序列长度>1）
│
├─ 分类头
│   ├─ BatchNorm1d(64)
│   ├─ ReLU()
│   ├─ Flatten()
│   ├─ 自适应维度处理
│   ├─ Linear(64→128)
│   ├─ Dropout(0.2)
│   └─ Linear(128→5)                 → 5类输出
```

### 特点

1. **深度残差网络架构**
   - 31个卷积层（1初始 + 1残差块 + 15×2层）
   - 大量残差连接防止梯度消失
   - BatchNorm加速训练并提高稳定性

2. **层次化特征学习**
   - 初始层快速下采样捕获全局特征
   - 深层残差块逐步细化特征表示
   - 渐进式池化减少时序维度

3. **正则化策略**
   - 多处使用Dropout(0.2)防止过拟合
   - BatchNorm提供额外正则化效果
   - 残差连接本身具有正则化作用

4. **自适应设计**
   - 智能处理不同长度的输入序列
   - 自适应特征维度调整
   - 防止池化操作过度降维

### 优点

- **强大的表达能力**：深度网络能够学习复杂的ECG模式
- **训练稳定**：残差连接和BatchNorm确保梯度有效传播
- **特征重用**：残差连接促进特征重用和组合
- **鲁棒性强**：多层Dropout提供强正则化，抗过拟合能力强
- **渐进式优化**：每层只需学习残差，优化更容易

### 缺点

- **计算复杂度高**：层数多，训练和推理时间较长
- **内存占用大**：需要存储中间特征用于残差连接
- **超参数敏感**：深度网络对学习率等超参数更敏感
- **可能过拟合**：在数据量不足时容易过拟合

## 使用建议

### 何时选择 Model_1

1. **资源受限场景**
   - 嵌入式设备部署
   - 实时处理要求高
   - 内存限制严格

2. **数据量有限**
   - 训练样本较少
   - 需要快速原型验证
   - 迁移学习基础模型

3. **简单分类任务**
   - 类别间差异明显
   - 特征相对简单
   - 不需要非常高的精度

### 何时选择 Model_2

1. **追求高精度**
   - 学术研究
   - 竞赛项目
   - 精度要求极高的应用

2. **数据充足**
   - 大规模标注数据集
   - 数据增强能力强
   - 有充足的训练时间

3. **复杂模式识别**
   - 细微的心律异常检测
   - 多种心律失常并存
   - 需要捕获长期依赖

## 训练建议

### Model_1 训练策略
- **学习率**：可使用较大的初始学习率（如0.001）
- **批大小**：32-128
- **优化器**：Adam或SGD with momentum
- **正则化**：主要依靠最后的Dropout

### Model_2 训练策略
- **学习率**：建议使用较小的学习率（如0.0001）
- **批大小**：16-64（内存限制）
- **优化器**：Adam或AdamW
- **学习率调度**：使用余弦退火或步长衰减
- **梯度裁剪**：防止梯度爆炸
- **早停机制**：防止过拟合

## 性能优化建议

1. **Model_1优化**
   - 可考虑添加深度可分离卷积减少参数
   - 使用SE注意力机制提升性能
   - 尝试不同的池化组合

2. **Model_2优化**
   - 使用分组卷积减少计算量
   - 考虑使用Ghost卷积
   - 实现模型并行以加速训练

## 总结

两种模型各有特色，Model_1注重效率和简单性，适合快速部署和资源受限场景；Model_2追求极致性能，适合复杂任务和精度要求高的场景。选择时需要根据具体的应用需求、数据情况、计算资源等因素综合考虑。